{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables to reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the exe path for chrome to open chrome page\n",
    "# Will open a chrome window\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mars 2020 Rover: T-Minus One Year and Counting \n",
      "Paragraph: The launch period for NASA's next rover, Mars 2020, opens exactly one year from today, July 17, 2020, and extends through Aug. 5, 2020.\n"
     ]
    }
   ],
   "source": [
    "# Visit the site to scrape\n",
    "# Will go to the website and extract the browser url\n",
    "news_url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(news_url)\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)\n",
    "\n",
    "# Find the actual website path we are going to scrape and read/show the data using BeautifulSoup\n",
    "news_html = browser.html\n",
    "soup = bs(news_html, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "news_title = soup.find('div', class_='content_title').text\n",
    "news_p = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "print (f'Title: {news_title}')\n",
    "print (f'Paragraph: {news_p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium picture: https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA17652_ip.jpg\n",
      "Large picture: https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA17652_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Visit the site to scrape\n",
    "# Will go to the website and extract the browser url\n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(jpl_url)\n",
    "time.sleep(1)\n",
    "\n",
    "# Find the actual website path we are going to scrape and read/show the data using BeautifulSoup\n",
    "jpl_html = browser.html\n",
    "soup = bs(jpl_html, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "image_link = soup.find('div',class_='carousel_container').article.footer.a['data-fancybox-href']\n",
    "featured_image_url_medium = f'https://www.jpl.nasa.gov{image_link}'\n",
    "print (f'Medium picture: {featured_image_url_medium}')\n",
    "\n",
    "time.sleep(1)\n",
    "full_image_elem = browser.find_by_id(\"full_image\")\n",
    "full_image_elem.click()\n",
    "\n",
    "time.sleep(1)\n",
    "more_info_elem = browser.find_link_by_partial_text('more info')\n",
    "more_info_elem.click()\n",
    "\n",
    "html = browser.html\n",
    "img_soup = bs(html, 'lxml')\n",
    "\n",
    "img_url_rel = img_soup.select_one('figure.lede a img').get(\"src\")\n",
    "\n",
    "featured_image_url_large = f'https://www.jpl.nasa.gov{img_url_rel}'\n",
    "\n",
    "print (f'Large picture: {featured_image_url_large}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 222 (2019-07-12) low -99.7ºC (-147.5ºF) high -24.8ºC (-12.6ºF)\n",
      "winds from the SSE at 4.2 m/s (9.4 mph) gusting to 15.6 m/s (34.8 mph)\n",
      "pressure at 7.60 hPapic.twitter.com/8Q8lyB6SjM\n"
     ]
    }
   ],
   "source": [
    "# Visit the site to scrape\n",
    "# Will go to the website and extract the browser url\n",
    "weather_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(weather_url)\n",
    "time.sleep(1)\n",
    "\n",
    "# Find the actual website path we are going to scrape and read/show the data using BeautifulSoup\n",
    "weather_html = browser.html\n",
    "soup = bs(weather_html, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "weather_all = soup.find_all('div', class_='js-tweet-text-container')\n",
    "\n",
    "weather_list = []\n",
    "for x in weather_all:\n",
    "    y = x.find('p', class_= 'js-tweet-text').text\n",
    "    if \"InSight\" in y:\n",
    "            weather_list.append(y)\n",
    "\n",
    "mars_weather = weather_list[0]\n",
    "print (weather_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the site to scrape\n",
    "# Will go to the website and extract the browser url\n",
    "facts_url = \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_url)\n",
    "\n",
    "# Find the actual website path we are going to scrape and read/show the data using BeautifulSoup\n",
    "facts_html = browser.html\n",
    "soup = bs(facts_html, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "facts_str = pd.read_html(facts_url)\n",
    "#facts_str[0]\n",
    "facts_str[1]\n",
    "\n",
    "# # https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.to_html.html\n",
    "facts_str[1].to_html(\"facts.html\", index = False, header = False)\n",
    "facts_html = facts_str[1].to_html(index = False, header = False)\n",
    "# facts_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the site to scrape\n",
    "# Will go to the website and extract the browser url\n",
    "hemisphere_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(hemisphere_url)\n",
    "\n",
    "# Find the actual website path we are going to scrape and read/show the data using BeautifulSoup\n",
    "hemisphere_html = browser.html\n",
    "soup = bs(hemisphere_html, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Find the links\n",
    "image_urls = [(a.text, a['href']) for a in browser.find_by_css('div[class=\"description\"] a')]\n",
    "#print (image_urls)\n",
    "\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for title,url in image_urls:\n",
    "    temp = {}\n",
    "    temp['title'] = title\n",
    "    browser.visit(url)\n",
    "    img_url = browser.find_by_css('img[class=\"wide-image\"]')['src']\n",
    "    temp['img_url'] = img_url\n",
    "    hemisphere_image_urls.append(temp)\n",
    "\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PythonData)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
